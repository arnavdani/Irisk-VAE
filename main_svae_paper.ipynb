{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import scipy as sc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dataset\n",
    "\n",
    "x = torch.tensor(\"data\") # actual import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denoiser\n",
    "class NoiseReduction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, rate_1_0, rate_0_1):\n",
    "        Y = np.where(self._gen_mask(X.shape, rate_1_0), np.zeros(X.shape), X)\n",
    "        Y = np.where(self._gen_mask(X.shape, rate_0_1), np.ones(X.shape), Y)\n",
    "        return Y\n",
    "    \n",
    "    def _gen_mask(self, shape, rate):\n",
    "        mask = []\n",
    "        for i in range(shape[0]):\n",
    "            ma = [1]*(int(shape[1]*rate)) + [0]*(int(shape[1]*(1-rate)))\n",
    "            np.random.shuffle(ma)\n",
    "            mask.append(ma)\n",
    "        return np.array(mask)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DVAE is deep stacked variational autoencoder\n",
    "class DVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(DVAE, self).__init__()\n",
    "        self.encoder_hidden_layers = nn.ModuleList()\n",
    "        self.decoder_hidden_layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        # Build encoder layers\n",
    "        for dim in hidden_dims:\n",
    "            if len(self.encoder_hidden_layers) == 0:\n",
    "                layer = nn.Linear(input_dim, dim)\n",
    "            else:\n",
    "                layer = nn.Linear(hidden_dims[-1], dim)\n",
    "\n",
    "            self.encoder_hidden_layers.append(layer)\n",
    "\n",
    "        # Build decoder layers\n",
    "        for i in range(len(hidden_dims) - 1, -1, -1):\n",
    "            if i == len(hidden_dims) - 1:\n",
    "                layer = nn.Linear(hidden_dims[i], input_dim)\n",
    "            else:\n",
    "                layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.decoder_hidden_layers.append(layer)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        z_mean = x\n",
    "        for layer in self.encoder_hidden_layers:\n",
    "            z_mean = layer(z_mean)\n",
    "            z_mean = nn.ReLU()(z_mean)\n",
    "\n",
    "        return z_mean\n",
    "\n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        epsilon = torch.randn_like(z_log_var)\n",
    "        z = z_mean + epsilon * torch.exp(0.5 * z_log_var)\n",
    "        return z\n",
    "\n",
    "    def decoder(self, z):\n",
    "        x_prime = z\n",
    "        for i, layer in enumerate(self.decoder_hidden_layers):\n",
    "            x_prime = layer(x_prime)\n",
    "            if i < len(self.decoder_hidden_layers) - 1:\n",
    "                x_prime = nn.ReLU()(x_prime)\n",
    "            else:\n",
    "                x_prime = nn.Sigmoid()(x_prime)\n",
    "        return x_prime\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean = self.encoder(x)\n",
    "        z_log_var = torch.randn_like(z_mean)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        x_prime = self.decoder(z)\n",
    "\n",
    "        # Normalize the output of the encoder\n",
    "        z = nn.functional.normalize(z, dim=0)\n",
    "\n",
    "        # Compute reconstruction loss\n",
    "        xent_loss = nn.functional.binary_cross_entropy(x_prime, x, reduction='mean')\n",
    "\n",
    "        # Compute KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "\n",
    "        # Compute total loss\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss, z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train DVAE\n",
    "def train_dvae(dvae, train_loader, optimizer, epochs=10, print_every=10):\n",
    "    dvae.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, _ = data\n",
    "            loss, _ = dvae(inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % print_every == print_every - 1:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / print_every))\n",
    "                running_loss = 0.0\n",
    "\n",
    "# outputs features\n",
    "\n",
    "def extract_features(self, x):\n",
    "    x = self.preprocess(x)\n",
    "    with torch.no_grad():\n",
    "        z = x\n",
    "        for i in range(len(self.ldim)):\n",
    "            z = self.encoder[i](z)\n",
    "            z = self.norm(z) # normalize output of encoder\n",
    "            z_mean = self.z_mean[i](z)\n",
    "            z_log_var = self.z_log_var[i](z)\n",
    "            eps = torch.randn_like(z_mean)\n",
    "            z = z_mean + torch.exp(0.5 * z_log_var) * eps\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVAE(nn.Module):\n",
    "    def __init__(self, dlayers, ddim, llayers, ldim, fdim, rho, beta):\n",
    "        super(SVAE, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        self.llayers = llayers\n",
    "        self.fdim = fdim\n",
    "        self.rho = rho\n",
    "        self.beta = beta\n",
    "\n",
    "        # Encoder\n",
    "        input_dim = fdim\n",
    "        for i in range(dlayers):\n",
    "            output_dim = ddim[i]\n",
    "            self.encoder_layers.append(nn.Linear(input_dim, output_dim))\n",
    "            input_dim = output_dim\n",
    "\n",
    "        # Variational hidden layers\n",
    "        self.z_mean_layers = nn.ModuleList()\n",
    "        self.z_log_var_layers = nn.ModuleList()\n",
    "        input_dim = ddim[-1]\n",
    "        for i in range(llayers):\n",
    "            output_dim = ldim[i]\n",
    "            self.z_mean_layers.append(nn.Linear(input_dim, output_dim))\n",
    "            self.z_log_var_layers.append(nn.Linear(input_dim, output_dim))\n",
    "            input_dim = output_dim\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layers.append(nn.Linear(ldim[-1], ddim[-1]))\n",
    "        input_dim = ddim[-1]\n",
    "        for i in range(dlayers-1, -1, -1):\n",
    "            output_dim = ddim[i]\n",
    "            self.decoder_layers.append(nn.Linear(input_dim, output_dim))\n",
    "            input_dim = output_dim\n",
    "        self.decoder_layers.append(nn.Linear(ddim[0], fdim))\n",
    "\n",
    "    def encode(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        z_mean = self.z_mean_layers[0](x)\n",
    "        z_log_var = self.z_log_var_layers[0](x)\n",
    "        for i in range(1, self.llayers):\n",
    "            z_mean = self.z_mean_layers[i](torch.relu(z_mean))\n",
    "            z_log_var = self.z_log_var_layers[i](torch.relu(z_log_var))\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "    def decode(self, z):\n",
    "        for layer in self.decoder_layers[:-1]:\n",
    "            z = torch.relu(layer(z))\n",
    "        x_prime = self.decoder_layers[-1](z)\n",
    "        return x_prime\n",
    "\n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        epsilon = torch.randn_like(z_log_var)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        x_prime = self.decode(z)\n",
    "        return x_prime, z_mean, z_log_var\n",
    "\n",
    "    def svae3_loss(self, x, x_prime, z_mean, z_log_var):\n",
    "        # Compute Xent loss\n",
    "        xent_loss = nn.functional.binary_cross_entropy(x_prime, x, reduction='sum')\n",
    "\n",
    "        # Compute KL divergence\n",
    "        kl_divergence = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    \n",
    "    def train_svae3(model, train_loader, learning_rate, rate_1_0, rate_0_1, cluster, epoch, min_delta, batch_size):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        train_loss = []\n",
    "        delta_loss = float('inf')\n",
    "\n",
    "        for e in range(epoch):\n",
    "            epoch_loss = 0\n",
    "            epoch_xent_loss = 0\n",
    "            epoch_sparsity_loss = 0\n",
    "            epoch_kl_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_idx, data in enumerate(train_loader):\n",
    "                x, _ = data\n",
    "                x = x.view(-1, SVAE.fdim)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                z_mean, z_log_var, z = model(x)\n",
    "\n",
    "                # Calculate reconstruction loss\n",
    "                x_recon = model.decoder(z)\n",
    "                xent_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / batch_size\n",
    "\n",
    "                # Calculate sparsity loss\n",
    "                rho_hat = torch.mean(z, dim=0)\n",
    "                sparsity_loss = SVAE.beta * torch.sum(SVAE.rho * torch.log(SVAE.rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))\n",
    "\n",
    "                # Calculate KL divergence loss\n",
    "                kl_loss = 0\n",
    "                for i in range(len(z_mean)):\n",
    "                    kl_loss += -0.5 * torch.sum(1 + z_log_var[i] - z_mean[i].pow(2) - z_log_var[i].exp())\n",
    "                kl_loss /= batch_size\n",
    "\n",
    "                # Calculate total loss\n",
    "                loss = xent_loss + rate_1_0 * F.relu(sparsity_loss - cluster) + rate_0_1 * kl_loss\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_xent_loss += xent_loss.item()\n",
    "                epoch_sparsity_loss += sparsity_loss.item()\n",
    "                epoch_kl_loss += kl_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            epoch_loss /= num_batches\n",
    "            epoch_xent_loss /= num_batches\n",
    "            epoch_sparsity_loss /= num_batches\n",
    "            epoch_kl_loss /= num_batches\n",
    "            train_loss.append(epoch_loss)\n",
    "\n",
    "            if e > 0 and (train_loss[e-1] - epoch_loss) < min_delta:\n",
    "                print(f\"Early stopping on epoch {e} due to loss decrease of {train_loss[e-1] - epoch_loss:.6f}\")\n",
    "                break\n",
    "\n",
    "            if e % 10 == 0:\n",
    "                print(f\"Epoch {e}, Loss: {epoch_loss:.6f}, Recon Loss: {epoch_xent_loss:.6f}, Sparsity Loss: {epoch_sparsity_loss:.6f}, KL Loss: {epoch_kl_loss:.6f}\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tesy SVAE\n",
    "def test_svae3(svae, test_loader):\n",
    "    \"\"\"\n",
    "    Tests the SVAE and returns the extracted features of the input tensor.\n",
    "\n",
    "    Args:\n",
    "        svae (SVAE): The SVAE object to test.\n",
    "        test_loader (torch.utils.data.DataLoader): The data loader for the test data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The extracted features of the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    svae.eval()\n",
    "\n",
    "    # Define empty list for storing extracted features\n",
    "    extracted_features = []\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over batches of test data\n",
    "        for batch_idx, (data, _) in enumerate(test_loader):\n",
    "\n",
    "            # Forward pass\n",
    "            z_mean, z_log_var, z = svae.extract_features(data)\n",
    "            output = svae.decoder(z)\n",
    "\n",
    "            # Store extracted features\n",
    "            extracted_features.append(z.detach().cpu())\n",
    "\n",
    "    # Concatenate features from all batches and return as tensor\n",
    "    return torch.cat(extracted_features, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
